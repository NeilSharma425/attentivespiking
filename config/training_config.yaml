# Training configuration

# Model architecture
model:
  vocab_size: 50257
  d_model: 768
  nhead: 12
  num_layers: 12
  dim_feedforward: 3072
  T: 4
  max_seq_len: 512
  dropout: 0.1

# Data
data:
  dataset: 'wikitext'
  batch_size: 8
  max_length: 512
  num_workers: 4

# Training
training:
  num_epochs: 20
  learning_rate: 3.0e-4
  weight_decay: 0.1
  grad_clip: 1.0
  
  # Loss weights
  alpha: 1.0
  beta: 0.1
  gamma: 0.05
  
  # Logging
  log_interval: 10
  save_interval: 5
  checkpoint_dir: 'checkpoints'
  log_dir: 'runs'
  
  # WandB
  use_wandb: false
  project_name: 'spike-adaptive-llm'
  run_name: null

# Device
device: 'cuda'